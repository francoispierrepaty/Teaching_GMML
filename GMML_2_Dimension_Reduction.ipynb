{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Vizualization and Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last time, we implemented PCA and Kernel PCA, which are two classical techniques to reduce the dimension of the data. Reducing the dimension of the data is not only useful as data processing, but also as a data vizualization tool. In this class, we will implement three classical algorithms in data vizualization and/or dimension reduction, based on geometrical methods:\n",
    "* Multidimension Scaling\n",
    "* Metric Multidimensional Scaling\n",
    "* Isomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "from sklearn.datasets import make_swiss_roll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multidimensional Scaling (MDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider a dataset with $n$ data. The features can be numerical or categorial, but we assume we can compute some dissimilarity value $D_{ij}$ between any pair of data $i$ and $j$. The goal is to recover some points $z_1, \\ldots, z_n \\in \\mathbb{R}^d$, with $d$ not too large (take $d=2$ for vizualization for example) such that the points $\\{z_i\\}$ represent \"well\" the dataset.\n",
    "\n",
    "What does it mean that it respresents \"well\" the dataset ? Imagine that the initial dataset is made of $n$ points $x_1, \\ldots, x_n \\in \\mathbb{R}^p$ (i.e. there are $p$ numerical features), and that the dissimilarities have been computed by $D_{ij} = \\|x_i - x_j\\|^2$. Defining $G \\in \\mathbb{R}^{n \\times n}$ the Gram matrix of the data, i.e. $G_{ij} = x_i^T x_j$, some basic computations show that:\n",
    "$$\n",
    "    G = -\\frac{1}{2}P D P\n",
    "$$\n",
    "where $P = I - \\frac{1}{n}1_n$ with $1_n$ the $n \\times n$ matrix with all coefficients equal to $1$.\n",
    "\n",
    "By computing $Z \\in \\mathbb{R}^{n \\times p}$ such that $G = Z Z^T$, and denoting by $z_i$ the $i$-th row of $Z$, it is clear that $G_{ij} = z_i^T z_j$. Note that since $G$ is of rank at most $p$, it is always possible to compute such a $Z$. We can compute $Z$ by performing the eigendecomposition of $G$, and drop all the zeros eigenvalues (if any).\n",
    "\n",
    "If we now want to reduce the dimension from $p$ to $d$, we can always compute the eigendecomposition of $G$, and keep only the $d$ largest eigenvalues of $G$. Note that if the dissimilarities were note computed from high-dimensional points, the matrix $G$ may have negative eigenvalues, in which case we can always set them to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider some synthetic data in $p=3$ dimension. This data is not linear, but clearly follows some one-dimensional pattern (the roll)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 400 # Number of points to consider (size of the dataset)\n",
    "X, y = make_swiss_roll(n_samples=n, noise=0.5) # X contains the point, y their color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = p3.Axes3D(fig)\n",
    "ax.view_init(7, -80)\n",
    "ax.scatter(X[:,0], X[:,1], X[:,2], 'o', cmap='hot', c=(y/np.max(y)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a `dissimilarity` function that will compute the dissimilarity matrix $D$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dissimilarity(data):\n",
    "    '''\n",
    "        Return the dissimilarity between the points in matrix `data` (n_samples, n_features).\n",
    "        Here, the dissimilarity is the squared Euclidean norm. It could be something else.\n",
    "    '''\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to \"center\" the data, i.e. compute the Gram matrix $G$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram(D):\n",
    "    '''\n",
    "        Return the Gram matrix associated with the dissimilarity matrix `D`.\n",
    "    '''\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the eigendecomposition of $G$, and keep the $d$ largest eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MDS(data, d=2):\n",
    "    '''\n",
    "        Compute the MDS of the dataset in matrix `data`(n_samples, n_features), with target dimension `d`.\n",
    "    '''\n",
    "    \n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In two dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = MDS(X, d=2)\n",
    "plt.scatter(X_reduced[:,0], X_reduced[:,1], cmap='hot', c=(y/np.max(y)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In one dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = MDS(X, d=1)\n",
    "plt.scatter(X_reduced, np.zeros(n), cmap='hot', c=(y/np.max(y)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Multidimensional Scaling (mMDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Metric MDS, we now seek to find some points $z_1, \\ldots, z_n \\in \\mathbb{R}^d$ such that\n",
    "$$\n",
    "    \\forall i,j,\\, D_{ij} \\approx \\|z_i - z_j\\|.\n",
    "$$\n",
    "\n",
    "More formally, we want to find the points $z_1, \\ldots, z_n \\in \\mathbb{R}^d$ that will minimize the loss (called here `stress`):\n",
    "$$\n",
    "    \\mathcal{S}(z_1, \\ldots, z_n) := \\sum_{i,j} \\left( D_{ij} - \\|z_i - z_j\\| \\right)^2.\n",
    "$$\n",
    "\n",
    "This is a non-convex problem, and should be difficult to optimize. But we can actually bound above the stress by a convex function $\\mathcal{L}$. We will minimize this convex loss $\\mathcal{L}$ instead of the stress $\\mathcal{S}$. This minimization can be done very efficiently using the SMACOF algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dissimilarity_sqrt(data):\n",
    "    '''\n",
    "        Return the Euclidean distance between the points in the dataset `data`.\n",
    "    '''\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def B(data, Delta):\n",
    "    '''\n",
    "        Return the matrix B in SMACOF iteration.\n",
    "    '''\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mMDS(data, d=2, N_iter=100, D_precomputed=None):\n",
    "    '''\n",
    "        Compute the mMDS of the dataset `data`, with target dimension `d` and `N_iter` iterations in SMACOF.\n",
    "    '''\n",
    "    n = data.shape[0]\n",
    "    \n",
    "    # Initialize at random\n",
    "    Z = np.random.normal(size=d*n).reshape([n,d])\n",
    "    \n",
    "    if D_precomputed is None:\n",
    "        D = dissimilarity_sqrt(X)\n",
    "    else:\n",
    "        D = D_precomputed\n",
    "    \n",
    "    # Run the algorithm\n",
    "    stress = []\n",
    "    for i in range(N_iter):\n",
    "        Z = # TODO\n",
    "        stress.append(np.sum(np.sum((D-dissimilarity_sqrt(Z))**2)))\n",
    "    \n",
    "    return Z, np.array(stress)/(n*n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In dimension 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z, stress = mMDS(X, d=2, N_iter=200)\n",
    "print('Stress value =', stress[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Z[:,0], Z[:,1], cmap='hot', c=(y/np.max(y)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4))\n",
    "plt.title('Convergence of SMACOF algorithm', fontsize=30)\n",
    "plt.plot(stress, lw=5)\n",
    "plt.ylabel('Stress', fontsize=25)\n",
    "plt.xlabel('Number of iterations', fontsize=25)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In dimension 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z, stress = mMDS(X, d=1, N_iter=200)\n",
    "print('Stress value =', stress[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Z, np.zeros(n), cmap='hot', c=(y/np.max(y)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4))\n",
    "plt.title('Convergence of SMACOF algorithm', fontsize=30)\n",
    "plt.plot(stress, lw=5)\n",
    "plt.ylabel('Stress', fontsize=25)\n",
    "plt.xlabel('Number of iterations', fontsize=25)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isomap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In MDS and metric MDS, the main problem is that the dissimilarities are computed as (squared) Euclidean distances. But clearly on the Swiss Roll, two points can be close in terms of Euclidean distance, but far away in terms of the geodesic distance on the Roll.\n",
    "\n",
    "Isomap is a tentative to solve this problem by:\n",
    "1. Computing dissimilarities that are not Euclidean, but \"geodesic\";\n",
    "2. Apply metric MDS to this dissimilarity matrix.\n",
    "\n",
    "The \"geodesic\" distance between the points in the dataset has to be thought this way. First, compute a graph of nearest neighbors, so that points that are close to each other are linked by an edge. Then, the \"geodesic\" distance between two points in the dataset is simply the (weighted) length of the shortest path in the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Nearest Neighbors Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each point $i$ in the dataset, we will compute its $k$ nearest neighbors and draw an edge between $i$ and all its nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-Nearest-Neighbors\n",
    "k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import kneighbors_graph\n",
    "graph = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the geodesic distance using Floydâ€“Warshall algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the geodesic graph distance using Floyd-Warshall algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.graph_shortest_path import graph_shortest_path\n",
    "geodesic_distance = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z, stress = # TODO\n",
    "print('Stress value =', stress[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Z[:,0], Z[:,1], cmap='hot', c=(y/np.max(y)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z, stress = # TODO\n",
    "print('Stress value =', stress[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Z, np.zeros(n), cmap='hot', c=(y/np.max(y)))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
